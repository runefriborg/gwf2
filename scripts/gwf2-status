#!/usr/bin/env python

import sys
import os
import os.path
import json
import argparse
import subprocess
import datetime
import logging
import tempfile

import gwf2
GWF_PATH = gwf2.__file__

from gwf2.process import RemoteProcess
from gwf2.reporting import ReportReader

from gwf2.colors import red, blue, green, yellow, bold

GWF_CONFIG_DIR = os.getenv('GWF_CONFIG_DIR', os.path.expanduser('~/.gwf2/'))

STATUS_COLORS = {'C': green, 'F': red, 'R': blue, 'Q': yellow}

if os.getenv('GWF_DEBUG', False):
    logging.basicConfig(level=logging.DEBUG)
else:
    logging.basicConfig(level=logging.CRITICAL)


def get_long_colored_status(status):
    long_status = {'C': 'completed', 'F': 'failed',
                   'R': 'running', 'Q': 'queued'}
    status_color_func = STATUS_COLORS[status]
    return status_color_func(long_status[status])


def get_short_colored_status(status):
    return STATUS_COLORS[status](status)


def get_job_environment(job):
    with open(os.path.join(GWF_CONFIG_DIR, 'jobs', job, 'environment')) as f:
        env = json.load(f)
        return env


def get_job_report(environment, job):
    """Fills up a reader for a specific job."""

    # if scratch_dir has not been written to the environment yet, the job
    # has been queued, but is not running.
    if not 'scratch_dir' in environment:
        return None

    log_path = os.path.join(GWF_CONFIG_DIR, 'jobs', job, 'log')

    if os.path.exists(log_path):
        return ReportReader(log_path)

    # if the log doesn't exist locally, it must still be on the mother node.
    remote_log_path = os.path.join(environment['scratch_dir'], 'log')

    logging.debug('fetch log file from {0}'.format(environment['mother_node']))
    process = RemoteProcess('cat {0}'.format(remote_log_path),
                            host=environment['mother_node'],
                            stdout=subprocess.PIPE)

    process.run()

    reader = ReportReader()
    for out in process.stdout:
        reader.write(out)

    process.wait()

    if process.returncode != 0:
        return None

    return reader


def get_task_log(environment, job, task_name, log):
    log_path = os.path.join(GWF_CONFIG_DIR, 'jobs',
                            job, task_name + '.' + log)

    try:
        with open(log_path) as log:
            return log.read()
    except:
        # if the log doesn't exist locally, it must still be on
        # the mother node.
        remote_log_path = os.path.join(environment['scratch_dir'],
                                       job, task_name + '.' + log)

        process = RemoteProcess('cat {0} 2> /dev/null || echo'.format(remote_log_path),
                                host=environment['mother_node'],
                                stdout=subprocess.PIPE)

        process.run()
        log = process.stdout.read()
        process.wait()

        return log


def reports(path):
    def comparator((j1, r1), (j2, r2)):
        if r1 is None:
            return -1
        if r2 is None:
            return 1
        if (r1.workflow['completed_at'] is None and
                r2.workflow['completed_at'] is None):
            if j1 == j2:
                return 0
            return -1 if j1 < j2 else 1
        elif r1.workflow['completed_at'] is None:
            return -1
        elif r2.workflow['completed_at'] is None:
            return 1
        else:
            if r1.workflow['completed_at'] == r2.workflow['completed_at']:
                return 0
            elif r1.workflow['completed_at'] < r2.workflow['completed_at']:
                return 1
            else:
                return -1

    reports = []
    if not os.path.exists(os.path.join(GWF_CONFIG_DIR, 'jobs')):
        return []
    for job in os.listdir(os.path.join(GWF_CONFIG_DIR, 'jobs')):
        environment = get_job_environment(job)
        reports.append((job, get_job_report(environment, job)))
    return sorted(reports, cmp=comparator)


def format_duration(td):
    hours, remainder = divmod(td.seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    return '{0:03d}:{1:02d}:{2:02d}'.format(hours, minutes, seconds)


def failed(failables):
    """Returns whether or not a failable thing has failed.

    A dict is failable if it contains the key `failure`."""
    if isinstance(failables, list):
        return any(failed(failable)
                   for failable in failables)
    return bool(failables['failure'])


def completed(completable):
    """Returns whether or not a completable thing has completed.

    A dict is completable if it contains the key `completed_at`."""
    return completable['completed_at'] is not None


def queued(queable):
    return queable['queued_at'] is not None and queable['started_at'] is None


def get_transfer_status(transfer):
    if failed(transfer):
        return 'F'
    if completed(transfer):
        return 'C'
    return 'R'


def get_task_status(task):
    if failed(task) or failed(task['transfers'].values()):
        return 'F'
    if completed(task):
        return 'C'
    return 'R'


def get_workflow_status(workflow):
    """Get status of workflow. If any task or transfer of the
    workflow has failed, the entire workflow has failed."""
    task_failed = any(get_task_status(task) == 'F'
                      for task in workflow['tasks'].values())
    if failed(workflow) or task_failed:
        return'F'
    if completed(workflow):
        return 'C'
    if queued(workflow):
        return 'Q'
    return 'R'


def get_duration(timeable):
    """Return the duration if a timeable thing.

    A dict is timeable if if contains the properties `started_at`
    and `completed_at`.

    If the has has completed, use its completion time to
    compute the duration. Otherwise, just compute how long it has
    been running."""
    started_at = timeable['started_at']
    if timeable['completed_at']:
        completed_at = timeable['completed_at']
    else:
        completed_at = datetime.datetime.today()
    return format_duration(completed_at - started_at)


def print_list():
    for job, report in reports(os.path.join(GWF_CONFIG_DIR, 'jobs')):
        # if there is no report, the job has just been queued, but no log file
        # has been written yet.
        if report is None:
            print bold(job), get_long_colored_status('Q')
            continue

        logging.debug(report.workflow)
        workflow_status = get_workflow_status(report.workflow)
        workflow_duration = get_duration(report.workflow)

        # Compute the number of tasks which have completed.
        completed_tasks = len([True
                               for task in report.workflow['tasks'].values()
                               if task['completed_at'] is not None])

        # Figure out the percentage of completed tasks. If zero tasks
        # were queued we'll get a ZeroDivisionError, but we just catch
        # it and use 100%.
        percentage = 100.0
        total_tasks = len(report.workflow['queued'])
        if total_tasks > 0:
            percentage = completed_tasks / float(total_tasks) * 100.0
        percentage_str = blue('{0:>3.0f}%'.format(percentage))

        template = '{0} {1} {2} {3} {4} {5} {6}'
        print template.format(bold(job),
                              get_long_colored_status(workflow_status),
                              workflow_duration,
                              completed_tasks,
                              len(report.workflow['queued']),
                              percentage_str,
                              report.workflow['file'])


def print_job(job):
    try:
        environment = get_job_environment(job)
        report = get_job_report(environment, job)
    except:
        print >> sys.stderr, "error: job does not exist or has not been started yet."
        sys.exit(1)

    workflow = report.workflow
    workflow_status = get_workflow_status(report.workflow)

    print bold("status    "), get_long_colored_status(workflow_status)
    print bold("file      "), workflow['file']
    print bold("queued    "), ' '.join(report.workflow['queued'])
    print bold("started   "), workflow['started_at']
    if workflow['completed_at']:
        print bold("completed "), workflow['completed_at']
    print bold("duration  "), get_duration(workflow)
    if workflow['failure']:
        print
        failure = workflow['failure']
        print bold(red('failure')), "({0})".format(failure['timestamp'])
        print "   ", failure['explanation']

    if not workflow['tasks']:
        return

    print
    print bold("tasks"), "(running, failed and completed)"
    max_width = max([len(x) for x in workflow['tasks'].keys()])
    template = "    {0} {1} {2}"
    for task_name, task in workflow['tasks'].iteritems():
        task_status = get_task_status(task)
        print template.format(bold(task_name.ljust(max_width)),
                              get_short_colored_status(task_status),
                              get_duration(task))


def print_transfers(transfers):
    for (src, dst), meta in transfers.iteritems():
        status = get_transfer_status(meta)
        print " " * 3, bold('status     '), get_long_colored_status(status)
        print " " * 3, bold('source     '), src
        print " " * 3, bold('destination'), dst
        print " " * 3, bold('duration   '), get_duration(meta)
        if meta['failure']:
            print
            print " " * 3, bold(red('failure')), '({0})'.format(meta['failure']['timestamp'])
            print " " * 7, meta['failure']['explanation']
        print


def print_task(job, task_name):
    try:
        environment = get_job_environment(job)
        report = get_job_report(environment, job)
    except:
        print >> sys.stderr, "error: job does not exist or has not been started yet."
        sys.exit(1)

    try:
        task = report.workflow['tasks'][task_name]
    except KeyError:
        print >> sys.stderr, "error: task does not exist or has not been started yet."
        sys.exit(1)

    status = get_task_status(task)
    print bold('status    '), get_long_colored_status(status)
    print bold('started   '), task['started_at']
    if task['completed_at']:
        print bold('completed '), task['completed_at']
    print bold('duration  '), get_duration(task)

    if task['failure']:
        failure = task['failure']
        print bold(red('failure')), "({0})".format(failure['timestamp'])
        print "   ", failure['explanation']

    if task['transfers']:
        print
        print bold('transfers')
        print_transfers(task['transfers'])

    print
    print bold(blue('stdout'))
    print get_task_log(environment, job, task_name, 'stdout')

    print bold(red('stderr'))
    print get_task_log(environment, job, task_name, 'stderr')


def plot(job, kind):
    try:
        environment = get_job_environment(job)
        report = get_job_report(environment, job)
    except:
        print >> sys.stderr, "error: job does not exist or has not been started yet."
        sys.exit(1)

    with tempfile.NamedTemporaryFile() as out:
        func, template = PLOTS[kind]
        func(out, job, report)

        # flush the output stream to disk
        out.flush()

        # get path to plot template
        template_path = os.path.join(os.path.dirname(GWF_PATH),
                                     'templates', template + '.plot')
        print template_path
        # now that we have written the data file, we must plot it with gnuplot
        call_template = 'gnuplot -e "infile=\'{0}\'; outfile=\'{1}\'" {2}'
        subprocess.call(call_template.format(out.name,
                                             template + '.png',
                                             template_path),
                        cwd=os.getcwd(),
                        shell=True)


def plot_timeline(out, job, report):
    workflow = report.workflow
    workflow_start_time = workflow['started_at']
    for task_name in workflow['queued']:
        if task_name not in workflow['tasks']:
            # task is still queued.
            print >> out, task_name, 0, 0, 0
        else:
            # task either failed, completed or is still running
            task = workflow['tasks'][task_name]
            status = get_task_status(task)

            if status == 'R':
                completed_time = datetime.datetime.now()
            elif status == 'C':
                completed_time = task['completed_at']
            else:
                completed_time = task['failure']['timestamp']

            min_y = task['started_at'] - workflow_start_time
            max_y = completed_time - workflow_start_time
            print >> out, task_name, 0, \
                min_y.total_seconds() / 3600, \
                max_y.total_seconds() / 3600


def plot_scheduling(out, job, report):
    workflow_started_at = None
    queued, running, completed, failed = 0, 0, 0, 0
    for timestamp, event, data in report.events:
        if event == 'WORKFLOW_STARTED':
            workflow_started_at = timestamp
            queued = len(data['queued'])
        elif event == 'TASK_STARTED':
            queued -= 1
            running += 1
        elif event == 'TASK_COMPLETED':
            running -= 1
            completed += 1
        elif event == 'TASK_FAILED':
            running -= 1
            failed += 1
        else:
            continue

        relative_time = timestamp - workflow_started_at
        print >> out, relative_time.total_seconds() / 3600, \
            queued, running, completed, failed

PLOTS = {
    'timeline': (plot_timeline, 'workflow_timeline'),
    'scheduling': (plot_scheduling, 'workflow_scheduling')
}

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-l', '--list', action='store_true',
                        help='list all previously and currently running jobs')
    parser.add_argument('-j', '--job',
                        help='show status for a specific job')
    parser.add_argument('-t', '--task',
                        help='show states for a specific task of a job')
    parser.add_argument('-p', '--plot', choices=PLOTS.keys(),
                        help='if a running job is given, plots the timeline ' +
                             'of the job using gnuplot')
    parser.add_argument('--no-color', action='store_true', default=False,
                        help='turn off colors for status output')

    args = parser.parse_args()

    if args.no_color:
        gwf.colors.off()

    # Add zeros to job id
    if args.job:
        args.job = "00000"[len(args.job):]+args.job
    if args.list and (args.task or args.job):
        parser.error('argument -l/--list: cannot be used with other arguments.')
    if args.task and not args.job:
        parser.error('argument -j/--job: required by argument -t/--task.')
    if args.plot and not args.job:
        parser.error('argument -p/--plot: required by argument -p/--plot.')

    if args.plot:
        plot(args.job, args.plot)
        sys.exit(0)

    if args.list:
        print_list()
    elif args.job and not args.task:
        print_job(args.job)
    elif args.job and args.task:
        print_task(args.job, args.task)
    else:
        print_list()
