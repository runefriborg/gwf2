#!/usr/bin/env python

import os
import sys
import argparse
import subprocess
import logging

from gwf.parser import parse
from gwf.dependency_graph import DependencyGraph
from gwf.task_scheduler import TaskScheduler
from gwf.process_scheduler import ProcessScheduler
from gwf.reporting import FileReporter

from gwf.environment import get_environment

GWF_DEFAULT_MAX_NODES = int(os.getenv('GWF_DEFAULT_MAX_NODES', '2'))
GWF_DEFAULT_WALLTIME = os.getenv('GWF_DEFAULT_WALLTIME', '48:0:0')
GWF_DEFAULT_QUEUE = os.getenv('GWF_DEFAULT_QUEUE', 'normal')


parser = argparse.ArgumentParser(description='Run a gwf workflow, locally or through submitting the tasks to an OpenPBS scheduler.')

#optgroup = parser.add_argument_group('optional arguments')
parser.add_argument('-l', '--local', default=False, action='store_true',
                    help='Run a workflow locally')
parser.add_argument('-f', '--file',
                    default='workflow.gwf', dest='workflow_file',
                    help='workflow file if not the default (workflow.gwf)')

exclgroup = parser.add_mutually_exclusive_group()
exclgroup.add_argument('-a', '--all', default=False, action='store_true',
                   help='run all end targets in the workflow (default)')
exclgroup.add_argument('-t', '--targets', nargs='+',
                   help='the target(s) to process.')

pbsgroup = parser.add_argument_group('pbs specific arguments', 'These options will affect the qsub parameters for the job submitted. Local executions, using (-l) ignores these options.')

pbsgroup.add_argument('-n', '--name',
                    help='name of the job')
pbsgroup.add_argument('-m', '--max-nodes', type=int,
                    default=GWF_DEFAULT_MAX_NODES,
                    help='the maximum number of nodes to use at any time')
pbsgroup.add_argument('-w', '--walltime', default=GWF_DEFAULT_WALLTIME,
                    help='duration of the entire pipeline in hours')
pbsgroup.add_argument('-q', '--queue-name', default=GWF_DEFAULT_QUEUE,
                    help='name of the queue to run the workflow in')
pbsgroup.add_argument('-v', '--dryrun', default=False, action='store_true',
                    help='Output qsub scripts without executing')

args = parser.parse_args()


# Create PBS JOB
if not args.local:
    template = '''#!/bin/sh
#PBS -N {name}
#PBS -q {queue_name}
#PBS -l nodes={max_nodes}:cores=16
#PBS -l walltime={walltime}
#PBS -d '''+os.getcwd()+'''

gwf --local '''+' '.join(sys.argv[1:])+'''
'''
    complete_script = template.format(**vars(args))

    if args.dryrun:
        print(complete_script)
    else:
        process = subprocess.Popen("qsub",
                               shell=True,
                               stdin=subprocess.PIPE)

        process.communicate(complete_script)

    sys.exit(0)

#####################################################################
####################### ACTUAL EXECUTION STARTING ###################
#####################################################################

# parse workflow file
workflow = parse(os.path.join(os.getcwd(), args.workflow_file))


# If no targets have been specified, then execute all end targets.
if not args.targets:
    args.all = True


# If all end targets should be run, we have to figure out what those
# are and set the target names in the workflow.
if args.all:
    graph = DependencyGraph(workflow)
    workflow.target_names = frozenset(node.task.name
                                      for node in graph.end_targets)
else:
    workflow.target_names = args.targets
    # check if all targets that we wish to run have been defined in the
    # workflow.
    for target in workflow.target_names:
        if target not in workflow.targets:
            print >> sys.stderr, 'target %s not found in workflow.' % target
            sys.exit(1)

# get a suitable environment for this workflow instance.
environment = get_environment()
logging.debug(environment)

# initialize file reporter such that it writes the log file to local
# storage (scratch) until the reporter is finalized.
local_dir = environment.scratch_dir
shared_dir = os.path.join(environment.config_dir, 'jobs', environment.job_id)

reporter = FileReporter(tmp_dir=local_dir, final_dir=shared_dir)

# initialize a scheduler for the workflow and start the run loop.
scheduler = TaskScheduler(environment,
                          reporter,
                          workflow,
                          ProcessScheduler())
scheduler.run()
