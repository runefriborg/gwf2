#!/usr/bin/env python

import os
import sys
import argparse
import subprocess
import logging
import json

import gwf.reporting

from gwf.parser import parse
from gwf.dependency_graph import DependencyGraph
from gwf.task_scheduler import TaskScheduler
from gwf.process_scheduler import ProcessScheduler
from gwf.reporting import FileReporter

from gwf.environment import get_environment

GWF_DEFAULT_MAX_NODES = int(os.getenv('GWF_DEFAULT_MAX_NODES', '2'))
GWF_DEFAULT_WALLTIME = os.getenv('GWF_DEFAULT_WALLTIME', '48:0:0')
GWF_DEFAULT_QUEUE = os.getenv('GWF_DEFAULT_QUEUE', 'normal')

GWF_CONFIG_DIR = os.getenv('GWF_CONFIG_DIR', os.path.expanduser('~/.gwf/'))

if os.getenv('GWF_DEBUG', False):
    logging.basicConfig(level=logging.DEBUG)
else:
    logging.basicConfig(level=logging.CRITICAL)

parser = argparse.ArgumentParser(description='Run a gwf workflow, locally or through submitting the tasks to an OpenPBS scheduler. Examples "gwf -f workflow.gwf -a" or "gwf -l -f workflow.gwf -t TargetB')

#optgroup = parser.add_argument_group('optional arguments')
parser.add_argument('-l', '--local', default=False, action='store_true',
                    help='Run a workflow locally')
parser.add_argument('-f', '--file', dest='workflow_file', required=True,
                    help='')

exclgroup = parser.add_mutually_exclusive_group()
exclgroup.add_argument('-t', '--targets', nargs='+',
                   help='the target(s) to process.')
exclgroup.add_argument('-a', '--all', default=False, action='store_true',
                   help='run all end targets in the workflow (default)')
parser.add_argument('-k', '--graph-key',
                    default=None,
                    help='')

pbsgroup = parser.add_argument_group('pbs specific arguments', 'These options will affect the qsub parameters for the job submitted. Local executions, using (-l) ignores these options.')

pbsgroup.add_argument('-n', '--name',
                    help='name of the job')
pbsgroup.add_argument('-m', '--max-nodes', type=int,
                    default=GWF_DEFAULT_MAX_NODES,
                    help='the maximum number of nodes to use at any time')
pbsgroup.add_argument('-w', '--walltime', default=GWF_DEFAULT_WALLTIME,
                    help='duration of the entire pipeline in hours')
pbsgroup.add_argument('-q', '--queue-name', default=GWF_DEFAULT_QUEUE,
                    help='name of the queue to run the workflow in')
pbsgroup.add_argument('-v', '--dryrun', default=False, action='store_true',
                    help='Output qsub scripts without executing')

args = parser.parse_args()


# If no targets have been specified, then execute all end targets.
if not args.targets:
    args.all = True


#####################################################################
####################### 1st WORKFLOW PARSING ########################
#####################################################################

# Perform 1st workflow parsing, output information and splitting workflows
if not args.graph_key:
    # parse workflow file
    workflow = parse(args.workflow_file)

    # If all end targets should be run, we have to figure out what those
    # are and set the target names in the workflow.
    if args.all:
        graph = DependencyGraph(workflow)
        workflow.target_names = frozenset(node.task.name
                                      for node in graph.end_targets)
    else:
        workflow.target_names = args.targets
        # check if all targets that we wish to run have been defined in the
        # workflow.
        for target in workflow.target_names:
            if target not in workflow.targets:
                print >> sys.stderr, 'target %s not found in workflow.' % target
                sys.exit(1)


    # build the dependency graph
    graph = DependencyGraph(workflow)

    # Retrieve the list of tasks to schedule
    targets_to_schedule = graph.schedule(workflow.target_names)

    # Retrieve the list of all tasks 
    tasklist = graph.tasklist(workflow.target_names)

    for t in tasklist:
        if t in targets_to_schedule:
            print("Run\t" + t.name)
        else:
            print("Skip\t" + t.name)


# Create PBS JOB
if not args.local:
    template = '''#!/bin/sh
#PBS -N {name}
#PBS -q {queue_name}
#PBS -l nodes={max_nodes}:ppn=16
#PBS -l walltime={walltime}
#PBS -d '''+os.getcwd()+'''

gwf --local '''+' '.join(sys.argv[1:])+'''
'''
    complete_script = template.format(**vars(args))

    if args.dryrun:
        print(complete_script)
    else:
        process = subprocess.Popen("qsub",
                                   shell=True,
                                   stdin=subprocess.PIPE,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)

        stdout, stderr = process.communicate(complete_script)
        if not stdout.strip().endswith('.in'):
            print stdout
            print stderr
            sys.exit(1)

        job_id = stdout.strip()

        shared_dir = os.path.join(GWF_CONFIG_DIR, 'jobs', job_id)
        os.makedirs(shared_dir)
        with open(os.path.join(shared_dir, 'environment'), 'w') as fp:
            json.dump({'job_id': job_id}, fp)
        print job_id

    sys.exit(0)


#####################################################################
####################### ACTUAL EXECUTION STARTING ###################
#####################################################################

# parse workflow file
workflow = parse(os.path.join(os.getcwd(), args.workflow_file))


# If all end targets should be run, we have to figure out what those
# are and set the target names in the workflow.
if args.all:
    graph = DependencyGraph(workflow)
    workflow.target_names = frozenset(node.task.name
                                      for node in graph.end_targets)
else:
    workflow.target_names = args.targets
    # check if all targets that we wish to run have been defined in the
    # workflow.
    for target in workflow.target_names:
        if target not in workflow.targets:
            print >> sys.stderr, 'target %s not found in workflow.' % target
            sys.exit(1)


# get a suitable environment for this workflow instance.
environment = get_environment()
logging.debug(environment)

shared_dir = os.path.join(environment.config_dir, 'jobs', environment.job_id)

# initialize file reporter such that it writes the log file to local
# storage (scratch) until the reporter is finalized.
reporter = FileReporter(tmp_dir=environment.scratch_dir, final_dir=shared_dir)
reporter.report(gwf.reporting.WORKFLOW_QUEUED)

# initialize a scheduler for the workflow and start the run loop.
scheduler = TaskScheduler(environment,
                          reporter,
                          workflow,
                          ProcessScheduler())
scheduler.run()
